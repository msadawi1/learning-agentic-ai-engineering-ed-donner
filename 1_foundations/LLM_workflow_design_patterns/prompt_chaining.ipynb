{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e622d64",
   "metadata": {},
   "source": [
    "# LLM Workflow Design Patterns: Prompt Chaining\n",
    "- This file is a project for applying design pattern: prompt-chaining\n",
    "- How to use:\n",
    "1. Fill the article.txt file with the article/text you would like to generate the quiz from\n",
    "2. Choose your LLM model (mine is gemini 2.5 flash)\n",
    "3. Change number of questions inside the main function\n",
    "\n",
    "``` python\n",
    "def main():\n",
    "    with open(\"./article.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        article = f.read()\n",
    "    summary = summarize(article)\n",
    "    keywords = extract_keywords(summary)\n",
    "    quiz = generate_quiz(keywords=keywords, num_questions=5) # change here\n",
    "    display_quiz(quiz)\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "![Prompt Chaining Diagram](../../assets/prompt_chaining.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f471457",
   "metadata": {},
   "source": [
    "## Project Idea: Article Summarizer and Quiz Generator\n",
    "\n",
    "##### Workflow (Prompt Chain)\n",
    "1. Step 1: Summarize\n",
    "2. Step 2: Extract Key Terms\n",
    "3. Step 3: Generate Quiz Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c49ee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1959d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini key loaded successfully\n",
      "Gemini URL loaded successfully\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "GEMINI_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if GEMINI_KEY:\n",
    "    print(\"Gemini key loaded successfully\")\n",
    "else:\n",
    "    print(\"Failed to load Gemini key.\")\n",
    "    exit(-1)\n",
    "\n",
    "GEMINI_URL = os.getenv(\"GOOGLE_API\")\n",
    "\n",
    "if GEMINI_URL:\n",
    "    print(\"Gemini URL loaded successfully\")\n",
    "else:\n",
    "    print(\"Failed to load Gemini URL.\")\n",
    "    exit(-1)\n",
    "\n",
    "client =  OpenAI(base_url=GEMINI_URL, api_key=GEMINI_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f23fdb",
   "metadata": {},
   "source": [
    "### Step 1: Summarize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb96b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text: str) -> str:\n",
    "    content = \"I will give you a text that you need to summarize, make sure to keep keywords and coherence of the text\\n\"\n",
    "    content += f\"The text: {text}\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": content}]\n",
    "\n",
    "    response = client.chat.completions.create(model=\"gemini-2.5-flash\", messages=messages)\n",
    "    summary = response.choices[0].message.content \n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb8b967",
   "metadata": {},
   "source": [
    "### Step 2: Extract keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b5a8c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def extract_keywords(summary: str) -> list:\n",
    "    content = f'I will give you a text summary, i need you to extract keywords regarding the topic for a quiz. Be precise and topic-oriented, and ignore general terms. Do not include any markdown. Respond with JSON, and only JSON, with the following format: {{\"keywords\": [keyword1, keyword2, ...]}}\\n'\n",
    "    content += f\"The summary: {summary}\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": content}]\n",
    "\n",
    "    response = client.chat.completions.create(model=\"gemini-2.5-flash\", messages=messages)\n",
    "    result = response.choices[0].message.content\n",
    "\n",
    "    keywords_dict = json.loads(result)\n",
    "\n",
    "    return keywords_dict[\"keywords\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476ea127",
   "metadata": {},
   "source": [
    "### Step 3: Generate quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48bc422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quiz(keywords: list, num_questions: int = 10) -> list:\n",
    "    content = f'I will give you a list of keywords, I want you to generate a {num_questions} question quiz about all keywords ({num_questions} questions total). Do not include any markdown or extra text or text styling. Return the quiz in a list of JSONs format [{{\"question\": ..., \"answer\": ...}}]\\n'\n",
    "    content += f\"The keywords list: {keywords}\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": content}]\n",
    "\n",
    "    response = client.chat.completions.create(model=\"gemini-2.5-flash\", messages=messages)\n",
    "\n",
    "    quiz_list = json.loads(response.choices[0].message.content)\n",
    "\n",
    "    return quiz_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e9af3d",
   "metadata": {},
   "source": [
    "### Display the quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e443d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_quiz(quiz: list):\n",
    "    for index, question in enumerate(quiz):\n",
    "        print(f\"Q{index + 1}: {question['question']}\")\n",
    "        print(f\"A: {question['answer']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a98d3ea",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50ada210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    with open(\"./article.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        article = f.read()\n",
    "\n",
    "    summary = summarize(article)\n",
    "\n",
    "    keywords = extract_keywords(summary)\n",
    "\n",
    "    quiz = generate_quiz(keywords=keywords, num_questions=5)\n",
    "\n",
    "    display_quiz(quiz)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f3238e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: What are LLMs, and how do 'tokens' and the 'limited context window' influence the 'inference' process?\n",
      "A: LLMs are large language models that process text in units called tokens. The limited context window restricts the total number of tokens (input + output) an LLM can consider at once, directly impacting the scope and coherence of the inference (the generation of output) by determining how much past information is available.\n",
      "\n",
      "Q2: Explain the relationship between 'prompt engineering,' 'context engineering,' and 'system instructions' in guiding an LLM's behavior.\n",
      "A: Prompt engineering involves crafting effective prompts for an LLM. Context engineering is a broader strategy that includes prompt engineering, but also encompasses providing relevant 'system instructions' (setting the model's persona, rules, or goals) and managing other input like 'message history' to establish the overall context and guide the LLM's behavior more effectively.\n",
      "\n",
      "Q3: How do 'tools' and 'external data' enhance the capabilities of an LLM beyond its pre-trained knowledge?\n",
      "A: Tools allow LLMs to perform actions or access 'external data' beyond their training. By using tools, an LLM can search the web, run code, interact with APIs, or retrieve specific information, thereby overcoming the limitations of its static training data and providing more accurate, up-to-date, or actionable responses.\n",
      "\n",
      "Q4: What role does 'message history' play when developing 'agents' powered by LLMs?\n",
      "A: Message history is crucial for LLM-powered agents as it provides the conversational context. It allows the agent to remember past interactions, maintain coherence over multiple turns, understand evolving user intent, and make informed decisions based on the ongoing dialogue, enabling more complex and stateful interactions.\n",
      "\n",
      "Q5: Briefly describe how 'prompt engineering' and managing the 'limited context window' are critical considerations when designing an LLM-powered 'agent' that uses 'tools' to access 'external data'.\n",
      "A: When designing an LLM agent, 'prompt engineering' is vital for instructing the agent on *when* and *how* to use its 'tools' to retrieve 'external data'. Managing the 'limited context window' is critical to ensure the agent retains enough of the 'message history' and the newly retrieved 'external data' to make informed decisions and maintain coherent interactions without losing track of previous turns or crucial information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
