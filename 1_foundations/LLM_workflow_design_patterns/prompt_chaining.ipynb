{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e622d64",
   "metadata": {},
   "source": [
    "# LLM Workflow Design Patterns: Prompt Chaining\n",
    "- This file is a project for applying design pattern: prompt-chaining\n",
    "\n",
    "![Prompt Chaining Diagram](../../assets/prompt_chaining.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f471457",
   "metadata": {},
   "source": [
    "## Project Idea: Article Summarizer and Quiz Generator\n",
    "\n",
    "##### Workflow (Prompt Chain)\n",
    "1. Step 1: Summarize\n",
    "2. Step 2: Extract Key Terms\n",
    "3. Step 3: Generate Quiz Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c49ee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b1959d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini key loaded successfully\n",
      "Gemini URL loaded successfully\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "GEMINI_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if GEMINI_KEY:\n",
    "    print(\"Gemini key loaded successfully\")\n",
    "else:\n",
    "    print(\"Failed to load Gemini key.\")\n",
    "    exit(-1)\n",
    "\n",
    "GEMINI_URL = os.getenv(\"GOOGLE_API\")\n",
    "\n",
    "if GEMINI_URL:\n",
    "    print(\"Gemini URL loaded successfully\")\n",
    "else:\n",
    "    print(\"Failed to load Gemini URL.\")\n",
    "    exit(-1)\n",
    "\n",
    "client =  OpenAI(base_url=GEMINI_URL, api_key=GEMINI_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f23fdb",
   "metadata": {},
   "source": [
    "### Step 1: Summarize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "abb96b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text: str) -> str:\n",
    "    content = \"I will give you a text that you need to summarize, make sure to keep keywords and coherence of the text\\n\"\n",
    "    content += f\"The text: {text}\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": content}]\n",
    "\n",
    "    response = client.chat.completions.create(model=\"gemini-2.5-flash\", messages=messages)\n",
    "    summary = response.choices[0].message.content \n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb8b967",
   "metadata": {},
   "source": [
    "### Step 2: Extract keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8b5a8c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def extract_keywords(summary: str) -> list:\n",
    "    content = f'I will give you a text summary, i need you to extract keywords regarding the topic for a quiz. Be precise and topic-oriented, and ignore general terms. Do not include any markdown. Respond with JSON, and only JSON, with the following format: {{\"keywords\": [keyword1, keyword2, ...]}}\\n'\n",
    "    content += f\"The summary: {summary}\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": content}]\n",
    "\n",
    "    response = client.chat.completions.create(model=\"gemini-2.5-flash\", messages=messages)\n",
    "    result = response.choices[0].message.content\n",
    "\n",
    "    keywords_dict = json.loads(result)\n",
    "\n",
    "    return keywords_dict[\"keywords\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476ea127",
   "metadata": {},
   "source": [
    "### Step 3: Generate quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "48bc422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quiz(keywords: list, num_questions: int = 10) -> list:\n",
    "    content = f'I will give you a list of keywords, I want you to generate a {num_questions} question quiz about all keywords ({num_questions} questions total). Do not include any markdown or extra text or text styling. Return the quiz in a list of JSONs format [{{\"question\": ..., \"answer\": ...}}]\\n'\n",
    "    content += f\"The keywords list: {keywords}\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": content}]\n",
    "\n",
    "    response = client.chat.completions.create(model=\"gemini-2.5-flash\", messages=messages)\n",
    "\n",
    "    quiz_list = json.loads(response.choices[0].message.content)\n",
    "\n",
    "    return quiz_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e9af3d",
   "metadata": {},
   "source": [
    "### Display the quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e443d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_quiz(quiz: list):\n",
    "    for index, question in enumerate(quiz):\n",
    "        print(f\"Q{index + 1}: {question['question']}\")\n",
    "        print(f\"A: {question['answer']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a98d3ea",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "50ada210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    with open(\"./article.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        article = f.read()\n",
    "\n",
    "    summary = summarize(article)\n",
    "\n",
    "    keywords = extract_keywords(summary)\n",
    "\n",
    "    quiz = generate_quiz(keywords=keywords, num_questions=5)\n",
    "\n",
    "    display_quiz(quiz)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3238e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: What are 'Context engineering' and 'Prompt engineering', and how do they relate to the 'LLM context'?\n",
      "A: 'Prompt engineering' focuses on crafting effective inputs to guide an LLM's response, while 'Context engineering' is the broader practice of managing and optimizing the entire 'LLM context' (including 'System instructions' and 'Message history') to achieve desired outcomes.\n",
      "\n",
      "Q2: Explain the relationship between 'Tokens', the 'Context window', and 'LLM constraints'.\n",
      "A: 'Tokens' are the basic units of text an LLM processes. The 'Context window' defines the maximum number of 'tokens' an LLM can process at once. Exceeding this limit is a primary 'LLM constraint', often leading to truncation or errors.\n",
      "\n",
      "Q3: How do 'System instructions' and 'Message history' contribute to the 'Dynamic context state' of an LLM?\n",
      "A: 'System instructions' establish the initial persona or guidelines for the LLM, and 'Message history' records the ongoing dialogue. Both dynamically update the 'LLM context' during a conversation, forming the 'Dynamic context state' that influences subsequent responses.\n",
      "\n",
      "Q4: In the context of 'AI agents', how do 'Tools' and 'External data' enhance their capabilities?\n",
      "A: 'AI agents' leverage 'Tools' to perform specific actions (e.g., search, calculations, API calls) and access 'External data' (e.g., databases, web content) to extend their knowledge and capabilities beyond their training, enabling more complex task execution.\n",
      "\n",
      "Q5: What is 'Applied AI', and how do effective 'LLM context' management and 'AI agents' contribute to its success?\n",
      "A: 'Applied AI' involves using AI technologies to solve real-world problems. Effective 'LLM context' management ensures models understand and respond appropriately, while 'AI agents' (often powered by LLMs and utilizing 'Tools' and 'External data') integrate various capabilities to automate complex tasks, making them crucial for successful 'Applied AI' solutions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
